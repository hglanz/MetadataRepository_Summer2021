---
title: "Test"
author: "Sucheen Sundaram"
date: "6/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(Frost2021Package)
library(XML)
library(purrr)
library(bestpredictor)
library(lubridate)
library(DT)
library(rdryad)
```

```{r}
test_vest <- read_html("https://datahub.io/higorspinto/car-sales-world-annual")
description <- test_vest %>% html_nodes(".readable-width") %>% html_text()
description %>%
  pluck(2) %>%
  str_remove_all("\n")
```

```{r}
url <- "https://www.kaggle.com/kaggle/meta-kaggle"
scrape_rvest(url, ".dataset-header-v2__title")
```

```{r}
url <- "https://datadryad.org/stash/dataset/doi:10.5061/dryad.mkkwh70xh"
  cols <- scrape_rvest(url, ".c-sidebox__heading , .o-heading__level2")
  data <- scrape_rvest(url, ".c-locations__data , .o-heading__level2+ p , .t-landing__text-wall p , #show_license p , .o-metrics__metric , .c-file-group , .c-file-group__summary")
  df <- data.frame(matrix(ncol = length(cols), nrow = 0))
  df <- rbind(df, data)
  file_info_columns <- scrape_rvest(url, "#sidebar a")
  file_info_data <- scrape_rvest(url, ".c-file-group__list div")
  if(!identical(file_info_data, character(0))) {
    file_info <- data.frame(filename = file_info_columns, sizeMB = file_info_data)
  }else{
    file_info <- c(NA)
  }
  if(!is.na(file_info)) {
    numbers <- parse_number(file_info$sizeMB)
    count <- 1
    for(i in file_info$sizeMB) {
      if("k" %in% i) { # if reported in kilobytes
        numbers[count] <- numbers[count]/1000
      }else if(!("MB" %in% i)) { # if reported in bytes
        numbers[count] <- numbers[count]/1000000
      }
      count <- count+1
    }
    file_info$sizeMB <- numbers
    file_info <- nest(file_info, data = everything())
  }
  names(df) <- cols[1:length(data)]
  df$`Data Files` <- file_info
  methods <- scrape_rvest(url, ".t-landing__text-wall:nth-child(8) p:nth-child(1)")
  if(!identical(methods, character(0))) {
    df$Methods <- paste(methods, collapse = ", ")
  }else{
    df$Methods <- c(NA)
  }
  authors <- paste(scrape_rvest(url, ".o-metadata__author"), collapse = "; ")
  if(!identical(authors, character(0))) {
    df$Authors <- authors
  }else{
    df$Authors <- c(NA)
  }
  date <- scrape_rvest(url, ".c-file-group__summary")
  if(identical(date, character(0))) {
    date <- scrape_rvest(url, ".o-metadata__group2-item:nth-child(1)")
    date <- str_split(date, ": ") %>% pluck(1)
    date <- date[2]
  }
  if(!identical(date, character(0))) {
    if(length(date) == 1) {
      df$Date <- date
    }else{
      df$Date <- date[1]
    }
  }else{
    df$Date <- c(NA)
  }
  affiliation <- scrape_rvest(url, ".o-metadata__affiliation")
  if(!identical(affiliation, character(0))) {
    df$AuthorAffiliation <- paste(affiliation, collapse = ", ")
  }else{
    df$AuthorAffiliation <- c(NA)
  }
target <- c("Citation", "Abstract", "Methods", "Data Files", "Authors", "AuthorAffiliation", "Date")
for(i in target) {
  if(!(i %in% names(df))) {
    df[i] <- c(NA)
  }
}
df[target]
```

```{r}
dataframe <- scrape_dryad("https://datadryad.org/stash/dataset/doi:10.5061/dryad.tqjq2bvxm")
pieces <- map_int(names(dataframe), ~str_split(dataframe[.x], " ") %>% pluck(1) %>% length())
dataframe[pieces != 2]
authors <- dataframe$Citation %>%
  str_split(" [(]") %>%
  pluck(1)
authors[1]
```

```{r}
# Iterate dryad
df <- data.frame(matrix(ncol = 5, nrow = 0))
count <- 0
for(i in 1:41864) {
  url <- paste("https://datadryad.org/search?page=", i, "&q=", sep = "")
  pg <- read_html(url)
  links <- html_attr(html_nodes(pg, "#documents a"), "href")
  links <- paste("https://datadryad.org", links, sep = "")
  links <- data.frame(links) %>% distinct()
  links <- links$links
  for(i in 1:length(links)) {
    df <- rbind(df, scrape_dryad(links[i]))
    print(nrow(df))
  }
}
file_info <- df$`Data Files`
file_info <- unnest(file_info, cols = c())
file_info
scrape_dryad("https://datadryad.org/stash/dataset/doi:10.5061/dryad.mkkwh70xh")
```


```{r}
url <- "https://dataverse.harvard.edu/dataverse/gemilang;jsessionid=a20dacca3200c53eeb823f127187"
name <- scrape_rvest(url, "#title")
  cols <- scrape_rvest(url, "#metrics-heading , th")
  data <- scrape_rvest(url, ".metrics-count-block , td")
  data <- data[data != "" & !str_detect(data, "dataDictionary")]
  df <- data.frame(matrix(ncol = length(cols), nrow = 0))
  df <- rbind(df, data)
  names(df) <- cols[1:ncol(df)]
  file_info_cols <- c("Name", "Downloads", "Variables", "Observations")
  file_info_names <- scrape_rvest(url, ".fileNameOriginal a")
  file_info_names <- file_info_names[file_info_names != ""]
  file_info <- data.frame(matrix(ncol = length(file_info_cols), nrow = length(file_info_names)+1))
  names(file_info) <- file_info_cols
  #file_info <- cbind(file_info, file_info_names)
  file_info$`FileName` <- checkNull(scrape_rvest(url, "#datasetForm\\:tabView\\:filesTable\\:0\\:fileInfoInclude-filesTable a"))
  fn <- data.frame(file_info$FileName) %>% distinct()
  fn <- fn$file_info.FileName
  file_info$FileName <- fn
  file_info <- file_info[, names(file_info) != "file_info_names"]
  downloads <- scrape_rvest(url, ".visible-lg-inline")
  variables <- scrape_rvest(url, ".unf-block span:nth-child(1)")
  observations <- scrape_rvest(url, ".unf-block span:nth-child(2)")
  if(!identical(downloads, character(0))) {
    while(length(downloads) != nrow(file_info)) {
      downloads <- append(downloads, NA)
    }
    file_info$Downloads <- parse_number(downloads)
  }else{
    file_info$Downloads <- NA
  }
  if(!identical(variables, character(0))) {
    while(length(variables) != nrow(file_info)) {
      variables <- append(variables, NA)
    }
    file_info$Variables <- parse_number(variables)
  }else{
    file_info$Variables <- NA
  }
  if(!identical(observations, character(0))) {
    while(length(observations) != nrow(file_info)) {
      observations <- append(observations, NA)
    }
    file_info$Observations <- parse_number(observations)
  }else{
    file_info$Observations <- NA
  }
  for(i in names(file_info)) {
    df[i] <- paste(c(file_info[i]), collapse = "; ")
  }
  df$Author <- checkNull(scrape_rvest(url, "#metadata_author td"))
  df$Name <- checkNull(name)
  df$`Deposit Date` <- checkNull(scrape_rvest(url, "#metadata_dateOfDeposit td"))
  df$Citation <- checkNull(scrape_rvest(url, ".citation-select"))
  target <- c("Name", "Description", "Subject", "Keyword", "Deposit Date", "Author", "Depositor", "Citation", "FileName", "Downloads", "Variables", "Observations")
  df$Depositor <- checkNull(scrape_rvest(url, "#metadata_depositor td"))
  for(i in target) {
    if(!(i %in% names(df))) {
      df[i] <- c(NA)
    }
  }
  df[target]
```

```{r}
full_df <- data.frame(matrix(ncol = 12, nrow = 0))
for(i in 36:11944) {
  url <- paste("https://dataverse.harvard.edu/dataverse/harvard?q=&types=dataverses%3Adatasets&sort=dateSort&order=desc&page=", i, sep = "")
  links <- Frost2021Package::links(url, ".card-title-icon-block a")
  links <- paste("https://dataverse.harvard.edu", links, sep = "")
  links <- links[str_detect(links, "/dataset")]
  for(j in links) {
    print(j)
    print(nrow(full_df))
    tryCatch(expr = {full_df <- rbind(full_df, scrape_harvard(j))}, error = function(error) {
      NULL
    })
    cat("\014")
  }
}
full_df <- rbind(target(read.csv("./Data/harvard_actual_scraped.csv"), names(full_df)), full_df)
```


```{r}
url <- "https://data.ca.gov/dataset/california-state-fleet"
desired_cols <- scrape_rvest(url, ".tags h3 , .module-heading , dt , .dataset-label")
cols <- scrape_rvest(url, ".module-heading , dt , .dataset-label , .tags h3")
data <- scrape_rvest(url, ".license span , .nav-item a , .module-shallow .heading , dd , .dataset-details , .well")
df <- data.frame(matrix(ncol = length(desired_cols), nrow = 0))
df <- rbind(df, data)
test <- scrape_cdph("https://data.ca.gov/dataset/covid-19-staff-data")
```

```{r}
full_df <- data.frame(matrix(ncol = ncol(test), nrow = 0))
names(full_df) <- names(test)
url <- "https://data.ca.gov/dataset?q=data&page=1"
pg <- read_html(url)
links <- html_attr(html_nodes(pg, "a"), "href")
links <- data.frame(links) %>%
  filter(str_detect(links, "/dataset/")) %>%
  distinct()
l <- vector(mode = "list", length = nrow(links))
for(i in links$links) {
  basic_link <- "https://data.ca.gov"
  data_link <- paste(basic_link, i, sep = "")
  full_df <- rbind(full_df, scrape_cdph(data_link))
}
df
```

```{r}
full_df <- data.frame(matrix(ncol = ncol(test), nrow = 0))
for(i in 1:119) {
  url <- paste("https://data.ca.gov/dataset?q=data&page=", i, sep = "")
  pg <- read_html(url)
  links <- paste("https://data.ca.gov", html_attr(html_nodes(read_html(url), ".dataset-heading a"), "href"), sep = "") %>% data.frame() %>% distinct()
  for(j in links$.) {
    data_link <- j
    tryCatch(expr = {
      full_df <- rbind(full_df, scrape_cdph(data_link))
    }, error = function(err) {
      NULL
    })
    print(nrow(full_df))
  }
}
clean_tag <- str_split(full_df$Tags, "[ ]+") %>%
  paste(sep = ";") %>%
  data.frame()
clean_tag <- str_remove_all(clean_tag$., c("c[{]", "[)]", "[\"]")) %>%
  data.frame()
```

```{r}
url <- "https://catalog.data.gov/dataset/boreas-tf-01-ssa-oa-weekly-tower-ch4-and-n2o-flux"
cols <- c(Frost2021Package::scrape_rvest(url, ".module-heading , .table-toggle-less tr+ tr .dataset-label , #access-use h3"), scrape_rvest(url, ".dataset-label"))
data <- c(scrape_rvest(url, ".module-narrow a"), scrape_rvest(url, ".table-toggle-less a , .dataset-details , td > span , #sec-dates td"))
cols <- cols %>% data.frame() %>% distinct()
cols <- cols$.
data <- data %>% data.frame() %>% rowid_to_column() %>% filter(rowid > 2)
data <- data$.
df <- data.frame(matrix(ncol = length(cols), nrow = 0))
df <- rbind(df, data)
names(df) <- cols[1:ncol(df)]
social <- scrape_rvest(url, ".social a")
string <- ""
for(i in social) {
  string <- paste(string, i, sep = " ")
}
string <- str_trim(string)
df$`Share on Social Sites` <- string
publisher <- scrape_rvest(url, "tr:nth-child(4) span")
publisher <- publisher[publisher != ""]
df$Publisher <- publisher
df$Contact <- scrape_rvest(url, ".contact a")
topics <- scrape_rvest(url, ".topics a")
topics <- topics %>% data.frame() %>% distinct()
topics <- topics$.
topics <- paste(topics, collapse = ", ")
df$Topics <- topics
df$`Terms of Use` <- scrape_rvest(url, ".terms a")[1]
access <- scrape_rvest(url, "#access-use strong , .access-public a")
access <- data.frame(access) %>% distinct()
access <- access$access
access <- str_remove_all(access, ":")
access <- paste(access, collapse = ", ")
df$`Access & Use Information` <- access
df$Tags <- checkNull(scrape_rvest(url, ".well"))
df$Name <- checkNull(scrape_rvest(url, ".prose h1"))
df$Description <- checkNull(scrape_rvest(url, ".embedded-content p"))
df$Maintainer <- checkNull(scrape_rvest(url, ".table-toggle-less a"))
target <- c("Name", "Description", "Metadata Created Date", "Metadata Updated Date", "Publisher", "Tags", "Category", "Maintainer")
for(i in target) {
    if(!(i %in% names(df))) {
      df[i] <- c(NA)
    }
  }
df[target]
#scrape_datagov("https://catalog.data.gov/dataset/department-for-the-aging-dfta-geriatric-mental-health-contracted-providers")
```

```{r}
# Iterate data.gov
full_df <- data.frame(matrix(ncol = 8, nrow = 0))
for(i in 1:15391) {
  url <- paste("https://catalog.data.gov/dataset?page=", i, sep = "")
  pg <- read_html(url)
  links <- html_attr(html_nodes(pg, ".dataset-heading a"), "href")
  links <- paste("https://catalog.data.gov", links, sep = "")
  links <- links %>% data.frame() %>% distinct()
  links <- links$.
  for(j in links) {
    scraped <- scrape_datagov(j)
    cat("\014")
    print(paste("Link:", j, "nrow =", nrow(full_df)))
    full_df <- rbind(full_df, scraped)
  }
}
```

```{r}
url <- "https://datahub.io/search"
link <- Frost2021Package::links(url, "a")
link <- link[str_detect(link, "/core/")]
link <- link %>% data.frame() %>% distinct()
link <- link$.
link <- paste("https://datahub.io", link, sep = "")
full_df <- data.frame(matrix(ncol = 8, nrow = 0))
name <- c()
for(i in link) {
  name <- append(name, checkNull(scrape_rvest(i, "h1") %>% paste(collapse = " ") %>% str_remove_all(" Certified")))
}
datahub$Name <- name
datahub
```

```{r}
# Zenodo
url <- "https://zenodo.org/record/5045610#.YOZWXRNKgTV"
name <- scrape_rvest(url, "h1")
author <- scrape_rvest(url, "h1 + p")
cols <- c(c("Name", "Authors"), scrape_rvest(url, ".addthis_32x32_style+ h4 , dt:nth-child(14) , dt:nth-child(12) , dt:nth-child(5) , dt:nth-child(1) , #collapse-stats tr:nth-child(6) td:nth-child(1) , #collapse-stats tr:nth-child(5) td:nth-child(1)"))
data <- scrape_rvest(url, "dd:nth-child(15) a , dd:nth-child(13) li , dd:nth-child(2) , #collapse-stats tr:nth-child(6) td:nth-child(2) , #collapse-stats tr:nth-child(5) td:nth-child(2) , h1+ p , h1 , #invenio-csl .ng-binding")
df <- data.frame(matrix(ncol = length(cols), nrow = 0))
df <- rbind(df, data)
keyword <- scrape_rvest(url, "dd:nth-child(6)")
communities <- scrape_rvest(url, "dd:nth-child(13) li")
file_info <- data.frame(Name = scrape_rvest(url, ".filename"), SizeMB = scrape_rvest(url, ".nowrap:nth-child(2)"))
numbers <- parse_number(file_info$SizeMB)
count <- 1
for(i in file_info$SizeMB) {
  if("kB" %in% i) {
    numbers[count] <- numbers[count]/1000
  }else if("GB" %in% i) {
    numbers[count] <- numbers[count]*1000
  }
}
file_info$SizeMB <- numbers
file_info <- file_info %>% nest(data = everything())
df <- mutate(df, FileInfo = file_info)
data <- append(data, file_info)
cols <- append(cols, "File Info")
names(df) <- cols[1:length(data)]
if(!identical(keyword, character(0)) & !str_detect(keyword, "Supplement")) {
  df$`Keyword(s):` <- keyword %>% str_trim()
}else{
  df$`Keyword(s):` <- c(NA)
}
if(!identical(communities, character(0))) {
  df$`Communities:` <- paste(communities, collapse = ", ")
}else{
  df$`Communities:` <- c(NA)
}
license <- paste(scrape_rvest(url, "dd:nth-child(15) a"), collapse = ", ")
if(!identical(license, character(0))) {
  df$`License (for files):` <- license
}else{
  df$`License (for files):` <- c(NA)
}
citation <- scrape_rvest(url, ".ng-binding")
if(!identical(citation, character(0))) {
  df$`Cite as` <- scrape_rvest(url, ".ng-binding")
}else{
  citation <- scrape_rvest(url, "#invenio-csl p")
  if(!identical(citation, character(0))) {
    df$`Cite as` <- citation
  }else{
    df$`Cite as` <- c(NA)
  }
}
df <- mutate(df, FileInfo <- file_info)
file_info <- unnest(file_info)
df <- mutate(df, FileName = paste(file_info$Name, collapse = ", "), SizeMB = paste(file_info$SizeMB, collapse = ", "))
df <- df[, c("Name", "Authors", "Unique views", "Unique downloads", "Publication date:", "Keyword(s):", "Communities:", "FileName", "SizeMB")]

scrape_zenodo("https://zenodo.org/record/5081153#.YOZkbi1h23U")
```

```{r}
full_df <- data.frame(matrix(ncol = 8, nrow = 0))
for(i in 1:1) {
  url <- paste("https://zenodo.org/search?page=", i, "&size=20#", sep = "")
  url <- "https://zenodo.org"
  pg <- read_html(url)
  links <- html_attr(html_nodes(pg, "h4 a"), "href")
  links <- links[str_detect(links, "/record/")]
  links <- paste("https://zenodo.org", links, sep = "")
  for(j in links) {
    full_df <- rbind(full_df, scrape_zenodo(j))
    print(nrow(full_df))
  }
}
```

```{r}
url <- "https://figshare.com/articles/journal_contribution/The_Inhibitors_to_Information_Seeking_Behavior_of_Nursing_Students_in_General_English_and_ESP_Courses_from_University_Instructors_Perspective_A_Case_Study/14999532"
name <- scrape_rvest(url, "._3lGK4")
description <- scrape_rvest(url, "._1dO13")
cols <- scrape_rvest(url, "._36trp , ._14z3R , ._1qu0d+ span")
categories <- scrape_rvest(url, "li")
keywords <- scrape_rvest(url, "._3v5nv span")
date <- scrape_rvest(url, "._1qu0d")
date <- checkNull(date)
exports <- scrape_rvest(url, "button span")
exports <- exports[exports != "Select an option"]
file_info_cols <- c("FileName", "Size")
file_info_data <- checkNull(scrape_rvest(url, "._1KU5g span"))
df <- data.frame(Name = checkNull(name), Description = checkNull(description), Categories = checkNull(paste(categories, collapse = ", ")), Keywords = checkNull(paste(keywords, collapse = ", ")), Date = checkNull(date), Exports = checkNull(paste(exports, collapse = ", ")))
count <- 1
for(i in file_info_cols) {
  df[i] <- checkNull(file_info_data[count])
  count <- count+1
}
df

Frost2021Package::scrape_figshare("https://figshare.com/articles/journal_contribution/The_Inhibitors_to_Information_Seeking_Behavior_of_Nursing_Students_in_General_English_and_ESP_Courses_from_University_Instructors_Perspective_A_Case_Study/14999532")
```

```{r}
# Iterate Figshare
link <- Frost2021Package::links("https://figshare.com/browse", ".WA1B-")
full_df <- data.frame(matrix(ncol = 8, nrow = 0))
for(i in link) {
  if(!str_detect(i, "https://")) {
    i <- paste("https:", i, sep = "")
  }
  print(i)
  print(nrow(full_df))
  tryCatch(expr = {
    full_df <- rbind(full_df, scrape_figshare(i))
  }, error = function(err) {
    NULL
  })
  cat("\014")
}
```


```{r}
url <- "https://www.re3data.org/repository/r3d100010129"
cols <- scrape_rvest(url, ".col-sm-3")[1:13]
  data <- scrape_rvest(url, ".col-sm-9")[1:13]
  df <- cbind(cols, data) %>% data.frame() %>% pivot_wider(names_from = cols, values_from = data)
  df$`Subject(s)` <- scrape_rvest(url, ".subjects") %>% paste(collapse = ", ")
  inst_cols <- scrape_rvest(url, ".content-block:nth-child(1) .col-sm-12:nth-child(11) .col-sm-3 , .content-block:nth-child(1) .col-sm-12:nth-child(9) .col-sm-3 , .content-block:nth-child(1) .col-sm-12:nth-child(7) .col-sm-3 , .content-block:nth-child(1) .col-sm-12:nth-child(1) .col-sm-3")[1:4]
  inst_cols <- inst_cols[inst_cols != "Type(s) of responsibility"]
  inst_data <- scrape_rvest(url, ".content-block:nth-child(1) .col-sm-12:nth-child(11) .col-sm-9 , .content-block:nth-child(1) .country , .content-block:nth-child(1) .col-sm-12:nth-child(1) .col-sm-9")
  df$Responsibility <- paste(checkNull(scrape_rvest(url, ".content-block:nth-child(1) .col-sm-12:nth-child(9) li")), collapse = ", ")
  df$Date <- checkNull(scrape_rvest(url, ".col-sm-12 .content-block .col-sm-12:nth-child(5) .col-sm-9"))
  if(is.na(df$`Name of repository`)) {
    df$Name.of.repository <- checkNull(scrape_rvest(url, "h1"))
  }
  df <- c(df, inst_data)
  df <- df %>% data.frame()
  df[c("Name.of.repository", "Repository.URL", "Subject.s.", "Description", "Contact", "Content.type.s.", "Keyword.s.", "Repository.type.s.", "Responsibility", "Date")]
```


```{r}
# Merge common data

corgis <- read.csv("./Data/CORGIS_metadata.csv")
cdph <- read.csv("./Data/cdph_scraped.csv")
datagov <- read.csv("./Data/harvard_scraped.csv")
datahub <- read.csv("./Data/datahub_scraped.csv")
uci <- read.csv("./Data/UCI_metadata.csv")
zenodo <- read.csv("./Data/zenodo_scraped.csv")
harvard <- read.csv("./Data/harvard_actual_scraped.csv")
re3data <- read.csv("./Data/re3data_scraped.csv")
datashare <- read.csv("./Data/datashare_scraped.csv")

mdf <- build_mdf(list(corgis, cdph, datagov, datahub, uci, zenodo, harvard, re3data, datashare))
mdf <- mdf[!str_detect(names(mdf), ".[:digit:]{1}")]
```

```{r}
# Shift and drop method function
col1 <- "Authors"
col2 <- "author"
for(i in 1:nrow(df)) {
  if(!is.na(df[i, col2]) & is.na(df[i, col1])) {
    df[i, col1] <- df[i, col2]
  }
}
df
```

```{r}
mdf <- merge_and_drop(mdf, "Authors", "author")
mdf <- merge_and_drop(mdf, "Authors", "Author")
mdf <- merge_and_drop(mdf, "Name", "title")
mdf <- merge_and_drop(mdf, "Publisher", "publisher")
mdf <- merge_and_drop(mdf, "file_size", "X.Size.")
mdf <- merge_and_drop(mdf, "file_size", "SizeMB")
mdf <- merge_and_drop(mdf, "Tags", "tags")
mdf <- merge_and_drop(mdf, "Tags", "Keyword.s..")
mdf <- merge_and_drop(mdf, "Created", "X.Created.")
mdf <- merge_and_drop(mdf, "Created", "date_created")
mdf <- merge_and_drop(mdf, "Created", "Metadata.Created.Date")
mdf <- merge_and_drop(mdf, "Last.Updated", "Metadata.Updated.Date")
mdf <- merge_and_drop(mdf, "Last.Updated", "X.Updated.")
for(i in 1:nrow(mdf)) {
  mdf[i, "Downloads"] <- str_split(mdf$Downloads, ", ") %>%
  pluck(i) %>%
  parse_number() %>%
  sum(na.rm = TRUE)
}
vec <- c()
for(i in 1:nrow(mdf)) {
  mdf[i, "Observations"] <- str_split(mdf$Observations, ", ") %>%
  pluck(i) %>%
  parse_number() %>%
  sum(na.rm = TRUE)
}
vec <- c()
for(i in 1:nrow(mdf)) {
  mdf[i, "Variables"] <- str_split(mdf$Variables, ", ") %>%
  pluck(i) %>%
  parse_number() %>%
  sum(na.rm = TRUE)
}
mdf <- merge_and_drop(mdf, "Variables", "ncols")
mdf <- merge_and_drop(mdf, "Observations", "nrows")
mdf <- clean(mdf)
for(i in 1:nrow(mdf)) {
  mdf[i, "file_size"] <- str_split(mdf$file_size, ", ") %>%
  pluck(i) %>%
  parse_number() %>%
  sum(na.rm = TRUE)
}
for(i in 1:nrow(mdf)) {
  if(mdf[i, "Variables"] == "0") {
    mdf[i, "Variables"] <- as.character(mdf[i, "nnum_vars"] + mdf[i, "nchar_vars"])
  }
}
cat("\014")
```

``` {r}
split <- str_split(mdf$Authors, ", ") %>%
  str_split(" and ") %>%
  str_split(", ")
for(i in 1:nrow(mdf)) {
  mdf[i, "num_authors"] <- split %>%
  pluck(i) %>%
  length()
}
```

```{r}
l <- c()
count <- 0
for(i in 1:11944) {
  if(count == nrow(harvard)) {
    break
  }
  url <- paste("https://dataverse.harvard.edu/dataverse/harvard?q=&types=dataverses%3Adatasets&sort=dateSort&order=desc&page=", i, sep = "")
  links <- Frost2021Package::links(url, ".card-title-icon-block a")
  links <- paste("https://dataverse.harvard.edu", links, sep = "")
  links <- links[str_detect(links, "/dataset")]
  for(j in links) {
    print(j)
    count <- count+1
    print(count)
    l <- append(l, j)
    #cat("\014")
  }
}
```


```{r}
mdf <- mdf[names(mdf) != "X.1" & names(mdf) != "rowid"]
for(i in 1:nrow(mdf)) {
  if(!is.na(mdf[i, "Authors"])) {
    if(str_detect(mdf[i, "Authors"], ";")) {
      split <- str_split(mdf[i, "Authors"], "; ")
    }else if(str_detect(mdf[i, "Authors"], ",")) {
      if(str_count(",") == 1) {
        split <- list("Null")
      }else{
        split <- str_split(mdf[i, "Authors"], ", ")
      }
    }else if(str_detect(mdf[i, "Authors"], "and")) {
      split <- str_split(mdf[i, "Authors"], " and ")
    }else{
      split <- list("Null")
    }
    mdf[i, "num_authors"] <- split %>% pluck(1) %>% length()
  }else{
    mdf[i, "num_authors"] <- c(NA)
  }
}

```

```{r}
# Iterate re3data
full_df <- data.frame(matrix(ncol = 10, nrow = 0))
for(i in 1:109) {
  link <- links(paste("https://www.re3data.org/search?query=&page=", i, sep = ""), "a")
  link <- link %>% data.frame() %>% distinct()
  link <- link$.
  link <- link[str_detect(link, "/repository") & !is.na(link)]
  link <- paste("https://re3data.org", link, sep = "")
  for(j in link) {
    print(j)
    print(nrow(full_df))
    tryCatch(expr = {
      full_df <- rbind(full_df, scrape_re3data(j))
    }, error = function(error) {
      NULL
    })
    cat("\014")
  }
}
```

```{r}
mdf <- merge_and_drop(mdf, "Name", "Name.of.repository")
mdf <- merge_and_drop(mdf, "Created", "Date")
mdf <- merge_and_drop(mdf, "Tags", "Keyword.s.")
mdf <- merge_and_drop(mdf, "Program.Contact.Name", "Contact")
mdf <- merge_and_drop(mdf, "Content.type.s.", "Subject")
mdf <- mdf[names(mdf) != "rowid"]
mdf <- merge_and_drop(mdf, "Tags", "Keyword")
mdf <- merge_and_drop(mdf, "Last.Updated", "Date.Available")
mdf <- merge_and_drop(mdf, "Description", "description")
mdf <- merge_and_drop(mdf, "file_size", "sizeMB")
mdf <- merge_and_drop(mdf, "Last.Updated", "date_submitted")
mdf <- clean(mdf)
mdf <- mdf[names(mdf) != "X"]
mdf <- merge_and_drop(mdf, "Views", "Unique.views")
mdf <- merge_and_drop(mdf, "Downloads", "Unique.downloads")
mdf <- merge_and_drop(mdf, "Name", "Name.of.repository")
```


```{r}
# Scrape Edinburgh
url <- "https://datashare.ed.ac.uk/handle/10283/1248"
cols <- c("Name", scrape_rvest(url, ".word-break h5"))
stats_url <- paste(url, "/statistics", sep = "")
df <- data.frame(matrix(ncol = length(cols), nrow = 0))
data <- scrape_rvest(url, "#aspect_artifactbrowser_ItemViewer_div_item-view .word-break , .first-page-header")
df <- rbind(df, data)
names(df) <- cols
for(i in names(df)) {
  df[i] <- str_remove_all(df[i], i)
}
df$Views <- scrape_rvest(stats_url, "#aspect_statistics_StatisticsTransformer_div_stats > .table-responsive:nth-child(2) .datacell") %>%
  parse_number() %>%
  sum()
df$`Top Country` <- checkNull(scrape_rvest(stats_url, ".table-responsive:nth-child(7) #aspect_statistics_StatisticsTransformer_cell_01"))
files <- scrape_rvest(url, ".col-sm-8 a")
files <- files[files != "" & !is.na(files)]
filename <- str_extract(files, "[:print:]+[(]{1}")
filename <- str_remove_all(filename, "[ (]")
filesize <- str_extract(files, "[(][:print:]+[)]")
df$filename <- paste(filename, collapse = "; ")
numbers <- parse_number(filesize)
count <- 1
for(i in numbers) {
  if(str_detect(i, "Kb")) {
    numbers[count] <- numbers[count]/1000
  }
  count <- count+1
}
df$sizeMB <- paste(parse_number(filesize), collapse = "; ")
df$Author <- checkNull(scrape_rvest(url, ".simple-item-view-creators")) %>%
  str_remove_all("Creator")
target(df, c("Name", "Author", "Date Available", "Description", "Top Country", "Citation", "Views", "filename", "sizeMB", "Type"))
```

```{r}
arr <- list(corgis, datahub)
map(arr, names)
build_mdf(arr)
```


```{r}
# Iterate datashare
full_df <- read.csv("./Data/datashare_scraped.csv")
for(i in 3:172) {
  url <- paste("https://datashare.ed.ac.uk/discover?rpp=20&etal=0&group_by=none&page=", i, sep = "")
  link <- Frost2021Package::links(url, ".artifact-description :nth-child(1)")
  link <- link[!is.na(link)]
  link <- paste("https://datashare.ed.ac.uk", link, sep = "")
  for(j in link) {
    print(j)
    print(nrow(full_df))
    full_df <- rbind(full_df, scrape_datashare(j))
    cat("\014")
  }
}
full_df
```

